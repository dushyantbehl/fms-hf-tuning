dataprocessor:
    type: default
    chat_template: |
      "{% for message in messages['messages'] %}\
        {% if message['role'] == 'user' %}{{ '<|user|>\n' + message['content'] + eos_token }}\
        {% elif message['role'] == 'system' %}{{ '<|system|>\n' + message['content'] + eos_token }}\
        {% elif message['role'] == 'assistant' %}{{ '<|assistant|>\n'  + message['content'] + eos_token }}\
        {% endif %}\
        {% if loop.last and add_generation_prompt %}{{ '<|assistant|>' }}\
        {% endif %}\
        {% endfor %}"  
datasets:
  - name: dataset_1
    sampling: 1.0
    data_paths:
      - /Volumes/Projects/Projects/ai-platform-engg/fms-hf-tuning/tests/artifacts/testdata/jsonl/multi_turn_chat.jsonl
    data_handlers:
      - name: apply_tokenizer_chat_template
        arguments:
          fn_kwargs:
            dataset_text_field: "formatted_chat"
      - name: tokenize
        arguments:
          batched: true
          fn_kwargs:
            dataset_text_field: "formatted_chat"
            truncation: False
            max_length: 4096
      - name: skip_large_text
        arguments:
          fn_kwargs:
            column_name: "input_ids"
            max_length: 4096
      - name: retain_columns
        arguments:
          columns:
          - "formatted_chat"