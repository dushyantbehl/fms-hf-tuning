{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c49a6d5",
   "metadata": {},
   "source": [
    "# Fine-tune using fms-hf-tuning with Alpaca Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b15eff",
   "metadata": {},
   "source": [
    "This example demonstrates how to fine-tune LLMs with the Alpaca Dataset using fms-hf-tuning utilizing `BuiltinTrainer` similar to torchtune from Kubeflow Trainer SDK.\n",
    "\n",
    "The demo is built by creating a new training runtime called fms-hf-tuning and also converting torchtune args to fms-hf-tuning args.\n",
    "\n",
    "Granite 4.0 350m: https://huggingface.co/ibm-granite/granite-4.0-350m/\n",
    "\n",
    "Alpaca Dataset: https://huggingface.co/datasets/tatsu-lab/alpaca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211fbf9",
   "metadata": {},
   "source": [
    "## List the training runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d35e4fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime(name='deepspeed-distributed', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='deepspeed', image='ghcr.io/kubeflow/trainer/deepspeed-runtime', num_nodes=1, device='Unknown', device_count='1'), pretrained_model=None)\n",
      "Runtime(name='fms-hf-tuning-runtime', trainer=RuntimeTrainer(trainer_type=<TrainerType.BUILTIN_TRAINER: 'BuiltinTrainer'>, framework='torchtune', image='fms-hf-tuning:test', num_nodes=1, device='gpu', device_count='2.0'), pretrained_model=None)\n",
      "Runtime(name='mlx-distributed', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='mlx', image='ghcr.io/kubeflow/trainer/mlx-runtime', num_nodes=1, device='Unknown', device_count='1'), pretrained_model=None)\n",
      "Runtime(name='torch-distributed', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='torch', image='fms-hf-tuning:test', num_nodes=1, device='Unknown', device_count='Unknown'), pretrained_model=None)\n",
      "Runtime(name='torchtune-qwen2.5-1.5b', trainer=RuntimeTrainer(trainer_type=<TrainerType.BUILTIN_TRAINER: 'BuiltinTrainer'>, framework='torchtune', image='ghcr.io/kubeflow/trainer/torchtune-trainer', num_nodes=1, device='gpu', device_count='2.0'), pretrained_model=None)\n"
     ]
    }
   ],
   "source": [
    "# List all available Kubeflow Training Runtimes.\n",
    "from kubeflow.trainer import *\n",
    "from kubeflow_trainer_api import models\n",
    "\n",
    "client = TrainerClient()\n",
    "for runtime in client.list_runtimes():\n",
    "    print(runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5f256",
   "metadata": {},
   "source": [
    "### Create PVCs for Models and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c11cc7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'api_version': 'v1',\n",
       " 'kind': 'PersistentVolumeClaim',\n",
       " 'metadata': {'annotations': None,\n",
       "              'creation_timestamp': datetime.datetime(2025, 11, 12, 21, 45, 7, tzinfo=tzutc()),\n",
       "              'deletion_grace_period_seconds': None,\n",
       "              'deletion_timestamp': None,\n",
       "              'finalizers': ['kubernetes.io/pvc-protection'],\n",
       "              'generate_name': None,\n",
       "              'generation': None,\n",
       "              'labels': None,\n",
       "              'managed_fields': [{'api_version': 'v1',\n",
       "                                  'fields_type': 'FieldsV1',\n",
       "                                  'fields_v1': {'f:spec': {'f:accessModes': {},\n",
       "                                                           'f:resources': {'f:requests': {'.': {},\n",
       "                                                                                          'f:storage': {}}},\n",
       "                                                           'f:volumeMode': {}}},\n",
       "                                  'manager': 'OpenAPI-Generator',\n",
       "                                  'operation': 'Update',\n",
       "                                  'subresource': None,\n",
       "                                  'time': datetime.datetime(2025, 11, 12, 21, 45, 7, tzinfo=tzutc())}],\n",
       "              'name': 'fms-hf-tuning-pvc',\n",
       "              'namespace': 'default',\n",
       "              'owner_references': None,\n",
       "              'resource_version': '1250085',\n",
       "              'self_link': None,\n",
       "              'uid': '42bc1863-e2ea-4020-91ec-8300b8bf7172'},\n",
       " 'spec': {'access_modes': ['ReadWriteOnce'],\n",
       "          'data_source': None,\n",
       "          'data_source_ref': None,\n",
       "          'resources': {'limits': None, 'requests': {'storage': '10Gi'}},\n",
       "          'selector': None,\n",
       "          'storage_class_name': 'standard',\n",
       "          'volume_attributes_class_name': None,\n",
       "          'volume_mode': 'Filesystem',\n",
       "          'volume_name': None},\n",
       " 'status': {'access_modes': None,\n",
       "            'allocated_resource_statuses': None,\n",
       "            'allocated_resources': None,\n",
       "            'capacity': None,\n",
       "            'conditions': None,\n",
       "            'current_volume_attributes_class_name': None,\n",
       "            'modify_volume_status': None,\n",
       "            'phase': 'Pending'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PersistentVolumeClaim for the fms-hf-tuning runtime.\n",
    "client.backend.core_api.create_namespaced_persistent_volume_claim(\n",
    "    namespace=\"default\",\n",
    "    body=models.IoK8sApiCoreV1PersistentVolumeClaim(\n",
    "        apiVersion=\"v1\",\n",
    "        kind=\"PersistentVolumeClaim\",\n",
    "        metadata=models.IoK8sApimachineryPkgApisMetaV1ObjectMeta(\n",
    "            name=\"fms-hf-tuning-pvc\"\n",
    "        ),\n",
    "        spec=models.IoK8sApiCoreV1PersistentVolumeClaimSpec(\n",
    "            accessModes=[\"ReadWriteOnce\"],\n",
    "            resources=models.IoK8sApiCoreV1VolumeResourceRequirements(\n",
    "                requests={\n",
    "                    \"storage\": models.IoK8sApimachineryPkgApiResourceQuantity(\"10Gi\")\n",
    "                }\n",
    "            ),\n",
    "        ),\n",
    "    ).to_dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67490f66",
   "metadata": {},
   "source": [
    "## Bootstrap LLM Fine-tuning Workflow\n",
    "\n",
    "Kubeflow TrainJob will train the model in the referenced (Cluster)TrainingRuntime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "641fae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = client.train(\n",
    "    runtime=client.get_runtime(name=\"fms-hf-tuning-runtime\"),\n",
    "    initializer=Initializer(\n",
    "        dataset=HuggingFaceDatasetInitializer(\n",
    "            storage_uri=\"hf://tatsu-lab/alpaca/data\"\n",
    "        ),\n",
    "        model=HuggingFaceModelInitializer(\n",
    "            storage_uri=\"hf://ibm-granite/granite-4.0-350M\",\n",
    "        )\n",
    "    ),\n",
    "    trainer=BuiltinTrainer(\n",
    "        config=TorchTuneConfig(\n",
    "            dataset_preprocess_config=TorchTuneInstructDataset(\n",
    "                source=DataFormat.PARQUET, split=\"train[:1000]\"\n",
    "            ),\n",
    "            resources_per_node={\n",
    "                \"memory\": \"4G\",\n",
    "                \"gpu\": 1,\n",
    "            },\n",
    "            \n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fbe8e",
   "metadata": {},
   "source": [
    "## Wait for running status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53eaa65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainJob(name='h871cd09097a', runtime=Runtime(name='fms-hf-tuning-runtime', trainer=RuntimeTrainer(trainer_type=<TrainerType.BUILTIN_TRAINER: 'BuiltinTrainer'>, framework='torchtune', image='fms-hf-tuning:test', num_nodes=1, device='gpu', device_count='2.0'), pretrained_model=None), steps=[Step(name='dataset-initializer', status='Succeeded', pod_name='h871cd09097a-dataset-initializer-0-0-tkh4s', device='Unknown', device_count='Unknown'), Step(name='model-initializer', status='Succeeded', pod_name='h871cd09097a-model-initializer-0-0-rjpv7', device='Unknown', device_count='Unknown'), Step(name='node-0', status='Running', pod_name='h871cd09097a-node-0-0-mmzr2', device='gpu', device_count='1')], num_nodes=1, creation_timestamp=datetime.datetime(2025, 11, 12, 21, 45, 51, tzinfo=TzInfo(0)), status='Running')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Wait for the running status.\n",
    "client.wait_for_job_status(name=job_name, status={\"Running\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a82b76",
   "metadata": {},
   "source": [
    "## Watch the TrainJob Logs\n",
    "\n",
    "### Dataset Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68e9d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-12T21:45:55Z INFO     [__main__.py:18] Starting dataset initialization\n",
      "2025-11-12T21:45:55Z INFO     [huggingface.py:28] Downloading dataset: tatsu-lab/alpaca\n",
      "2025-11-12T21:45:55Z INFO     [huggingface.py:29] ----------------------------------------\n",
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00,  3.10it/s]\n",
      "2025-11-12T21:45:57Z INFO     [huggingface.py:41] Dataset has been downloaded\n"
     ]
    }
   ],
   "source": [
    "from kubeflow.trainer.constants import constants\n",
    "\n",
    "for line in client.get_job_logs(job_name, follow=True, step=constants.DATASET_INITIALIZER):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f970c03",
   "metadata": {},
   "source": [
    "### Model Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ce8c1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-12T21:45:55Z INFO     [__main__.py:17] Starting pre-trained model initialization\n",
      "2025-11-12T21:45:55Z INFO     [huggingface.py:26] Downloading model: ibm-granite/granite-4.0-350M\n",
      "2025-11-12T21:45:55Z INFO     [huggingface.py:27] ----------------------------------------\n",
      "Fetching 8 files: 100%|██████████| 8/8 [00:15<00:00,  1.96s/it]\n",
      "2025-11-12T21:46:12Z INFO     [huggingface.py:43] Model has been downloaded\n"
     ]
    }
   ],
   "source": [
    "for line in client.get_job_logs(job_name, follow=True, step=constants.MODEL_INITIALIZER):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67775ea",
   "metadata": {},
   "source": [
    "### Trainer Node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae672d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: dataset-initializer, Status: Succeeded, Devices: Unknown x Unknown\n",
      "\n",
      "Step: model-initializer, Status: Succeeded, Devices: Unknown x Unknown\n",
      "\n",
      "Step: node-0, Status: Running, Devices: gpu x 1\n",
      "\n",
      "Rank-0 [INFO]:sft_trainer.py:main: fms-hf-tuning execution start\n",
      "Rank-0 [INFO]:sft_trainer.py:main: \n",
      "========================= Flat Arguments =========================\n",
      "---------------------------- Model Arguments -----------------------\n",
      "  embedding_size_multiple_of : 1\n",
      "  flash_attn_implementation  : flash_attention_2\n",
      "  model_name_or_path         : ibm-granite/granite-4.0-350M\n",
      "  tokenizer_name_or_path     : None\n",
      "  torch_dtype                : torch.bfloat16\n",
      "  use_flash_attn             : False\n",
      "---------------------------- Data Arguments -----------------------\n",
      "  add_special_tokens         : None\n",
      "  chat_template              : None\n",
      "  data_config_path           : None\n",
      "  data_formatter_template    : None\n",
      "  dataset_conversation_field : None\n",
      "  dataset_image_field        : None\n",
      "  dataset_text_field         : text\n",
      "  do_dataprocessing_only     : False\n",
      "  instruction_template       : None\n",
      "  num_eval_dataset_shards    : 1\n",
      "  num_train_dataset_shards   : 1\n",
      "  response_template          : None\n",
      "  training_data_path         : tatsu-lab/alpaca\n",
      "  validation_data_path       : None\n",
      "---------------------------- Training Arguments -----------------------\n",
      "  __cached__setup_devices                 : cuda:0\n",
      "  _n_gpu                                  : 1\n",
      "  accelerator_config                      : AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False)\n",
      "  adafactor                               : False\n",
      "  adam_beta1                              : 0.9\n",
      "  adam_beta2                              : 0.999\n",
      "  adam_epsilon                            : 1e-08\n",
      "  auto_find_batch_size                    : False\n",
      "  average_tokens_across_devices           : False\n",
      "  batch_eval_metrics                      : False\n",
      "  bf16                                    : False\n",
      "  bf16_full_eval                          : False\n",
      "  cache_dir                               : None\n",
      "  data_seed                               : None\n",
      "  dataloader_drop_last                    : False\n",
      "  dataloader_num_workers                  : 0\n",
      "  dataloader_persistent_workers           : False\n",
      "  dataloader_pin_memory                   : True\n",
      "  dataloader_prefetch_factor              : None\n",
      "  ddp_backend                             : None\n",
      "  ddp_broadcast_buffers                   : None\n",
      "  ddp_bucket_cap_mb                       : None\n",
      "  ddp_find_unused_parameters              : None\n",
      "  ddp_timeout                             : 1800\n",
      "  debug                                   : []\n",
      "  deepspeed                               : None\n",
      "  deepspeed_plugin                        : None\n",
      "  disable_tqdm                            : False\n",
      "  distributed_state                       : Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "  do_eval                                 : False\n",
      "  do_predict                              : False\n",
      "  do_train                                : False\n",
      "  enable_reduce_loss_sum                  : False\n",
      "  eval_accumulation_steps                 : None\n",
      "  eval_delay                              : 0\n",
      "  eval_do_concat_batches                  : True\n",
      "  eval_on_start                           : False\n",
      "  eval_steps                              : None\n",
      "  eval_strategy                           : no\n",
      "  eval_use_gather_object                  : False\n",
      "  fp16                                    : False\n",
      "  fp16_backend                            : auto\n",
      "  fp16_full_eval                          : False\n",
      "  fp16_opt_level                          : O1\n",
      "  fsdp                                    : []\n",
      "  fsdp_config                             : {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\n",
      "  fsdp_min_num_params                     : 0\n",
      "  fsdp_transformer_layer_cls_to_wrap      : None\n",
      "  full_determinism                        : False\n",
      "  gradient_accumulation_steps             : 1\n",
      "  gradient_checkpointing                  : True\n",
      "  gradient_checkpointing_kwargs           : None\n",
      "  greater_is_better                       : None\n",
      "  group_by_length                         : False\n",
      "  half_precision_backend                  : auto\n",
      "  hub_always_push                         : False\n",
      "  hub_model_id                            : None\n",
      "  hub_private_repo                        : None\n",
      "  hub_revision                            : None\n",
      "  hub_strategy                            : every_save\n",
      "  hub_token                               : None\n",
      "  ignore_data_skip                        : False\n",
      "  include_for_metrics                     : []\n",
      "  include_inputs_for_metrics              : False\n",
      "  include_num_input_tokens_seen           : False\n",
      "  include_tokens_per_second               : False\n",
      "  jit_mode_eval                           : False\n",
      "  label_names                             : None\n",
      "  label_smoothing_factor                  : 0.0\n",
      "  learning_rate                           : 1e-05\n",
      "  length_column_name                      : length\n",
      "  liger_kernel_config                     : None\n",
      "  load_best_model_at_end                  : False\n",
      "  local_rank                              : 0\n",
      "  log_level                               : info\n",
      "  log_level_replica                       : warning\n",
      "  log_on_each_node                        : True\n",
      "  logging_dir                             : /initializer/tmp/runs/Nov12_21-46-23_h871cd09097a-node-0-0\n",
      "  logging_first_step                      : False\n",
      "  logging_nan_inf_filter                  : True\n",
      "  logging_steps                           : 5\n",
      "  logging_strategy                        : steps\n",
      "  lr_scheduler_kwargs                     : {}\n",
      "  lr_scheduler_type                       : linear\n",
      "  max_grad_norm                           : 1.0\n",
      "  max_seq_length                          : 1024\n",
      "  max_steps                               : -1\n",
      "  metric_for_best_model                   : None\n",
      "  mp_parameters                           : \n",
      "  neftune_noise_alpha                     : None\n",
      "  no_cuda                                 : False\n",
      "  num_train_epochs                        : 1.0\n",
      "  optim                                   : adamw_torch\n",
      "  optim_args                              : None\n",
      "  optim_target_modules                    : None\n",
      "  output_dir                              : /initializer/tmp\n",
      "  overwrite_output_dir                    : False\n",
      "  packing                                 : True\n",
      "  past_index                              : -1\n",
      "  per_device_eval_batch_size              : 8\n",
      "  per_device_train_batch_size             : 1\n",
      "  per_gpu_eval_batch_size                 : None\n",
      "  per_gpu_train_batch_size                : None\n",
      "  prediction_loss_only                    : False\n",
      "  push_to_hub                             : False\n",
      "  push_to_hub_model_id                    : None\n",
      "  push_to_hub_organization                : None\n",
      "  push_to_hub_token                       : None\n",
      "  ray_scope                               : last\n",
      "  remove_unused_columns                   : True\n",
      "  report_to                               : []\n",
      "  restore_callback_states_from_checkpoint : False\n",
      "  resume_from_checkpoint                  : None\n",
      "  run_name                                : None\n",
      "  save_model_dir                          : /initializer/tmp\n",
      "  save_on_each_node                       : False\n",
      "  save_only_model                         : False\n",
      "  save_safetensors                        : True\n",
      "  save_steps                              : 500\n",
      "  save_strategy                           : epoch\n",
      "  save_total_limit                        : None\n",
      "  seed                                    : 42\n",
      "  skip_memory_metrics                     : True\n",
      "  tf32                                    : None\n",
      "  torch_compile                           : False\n",
      "  torch_compile_backend                   : None\n",
      "  torch_compile_mode                      : None\n",
      "  torch_empty_cache_steps                 : None\n",
      "  torchdynamo                             : None\n",
      "  tpu_metrics_debug                       : False\n",
      "  tpu_num_cores                           : None\n",
      "  trackers                                : ['file_logger']\n",
      "  use_cpu                                 : False\n",
      "  use_ipex                                : False\n",
      "  use_legacy_prediction_loop              : False\n",
      "  use_liger_kernel                        : False\n",
      "  use_mps_device                          : False\n",
      "  warmup_ratio                            : 0.0\n",
      "  warmup_steps                            : 0\n",
      "  weight_decay                            : 0.0\n",
      "---------------------------- QLoRA Config -----------------------\n",
      "  auto_gptq : None\n",
      "  bnb_qlora : None\n",
      "---------------------------- AADP (fms-acceleration) Config -----------------------\n",
      "  multipack    : None\n",
      "  padding_free : None\n",
      "---------------------------- Fused Ops Kernels Config -----------------------\n",
      "  fast_kernels : None\n",
      "  fused_lora   : None\n",
      "---------------------------- Fast MoE Config -----------------------\n",
      "  fast_moe : None\n",
      "---------------------------- Tracker Config -----------------------\n",
      "  aim_experiment          : fms-hf-tuning\n",
      "  aim_remote_server_ip    : None\n",
      "  aim_remote_server_port  : None\n",
      "  aim_repo                : None\n",
      "  aim_url                 : None\n",
      "  clearml_project         : fms-hf-tuning\n",
      "  clearml_task            : SFTTrainer\n",
      "  experiment              : fms-hf-tuning\n",
      "  mlflow_experiment       : fms-hf-tuning\n",
      "  mlflow_tracking_uri     : None\n",
      "  run_uri_export_path     : None\n",
      "  scanner_output_filename : scanner_output.json\n",
      "  training_logs_filename  : training_logs.jsonl\n",
      "---------------------------- Trainer Controller Config -----------------------\n",
      "  trainer_controller_config_file : None\n",
      "========================= Arguments Done =========================\n",
      "\n",
      "Rank-0 [INFO]:sft_trainer.py:train: using the output directory at /initializer/tmp\n",
      "Rank-0 [INFO]:tracker_factory.py:_register_trackers: Registering trackers\n",
      "Rank-0 [WARNING]:tracker_factory.py:_register_aim_tracker: Not registering Aimstack tracker due to unavailablity of package.\n",
      "Please install aim if you intend to use it.\n",
      "\t pip install aim\n",
      "Rank-0 [INFO]:tracker_factory.py:_register_file_logging_tracker: Registered file logging tracker\n",
      "Rank-0 [WARNING]:tracker_factory.py:_register_mlflow_tracker: Not registering mlflow tracker due to unavailablity of package.\n",
      "Please install mlflow if you intend to use it.\n",
      "\t pip install mlflow\n",
      "Rank-0 [WARNING]:tracker_factory.py:_register_hf_resource_scanner_tracker: Not registering HFResourceScanner tracker due to unavailablity of package.\n",
      "Please install HFResourceScanner if you intend to use it.\n",
      "\t pip install HFResourceScanner\n",
      "Rank-0 [WARNING]:tracker_factory.py:_register_clearml_tracker: Not registering clearml tracker due to unavailablity of package.\n",
      "Please install clearml if you intend to use it.\n",
      "\t pip install clearml\n",
      "Rank-0 [INFO]:sft_trainer.py:train: Loading the model ibm-granite/granite-4.0-350M now\n",
      "Rank-0 [INFO]:sft_trainer.py:train: Trying to load ibm-granite/granite-4.0-350M as vision model\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:2199: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--ibm-granite--granite-4.0-350M/snapshots/bd8a1497065c0d6ba1ef19af6b0d2b14bacf71c2/config.json\n",
      "Model config GraniteMoeHybridConfig {\n",
      "  \"architectures\": [\n",
      "    \"GraniteMoeHybridForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_multiplier\": 0.015625,\n",
      "  \"bos_token_id\": 100257,\n",
      "  \"embedding_multiplier\": 12,\n",
      "  \"eos_token_id\": 100257,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_method\": \"mup\",\n",
      "  \"initializer_range\": 0.1,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_types\": [\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\"\n",
      "  ],\n",
      "  \"logits_scaling\": 4,\n",
      "  \"mamba_chunk_size\": 256,\n",
      "  \"mamba_conv_bias\": true,\n",
      "  \"mamba_d_conv\": 4,\n",
      "  \"mamba_d_head\": 16,\n",
      "  \"mamba_d_state\": 256,\n",
      "  \"mamba_expand\": 2,\n",
      "  \"mamba_n_groups\": 1,\n",
      "  \"mamba_n_heads\": 128,\n",
      "  \"mamba_proj_bias\": false,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"granitemoehybrid\",\n",
      "  \"normalization_function\": \"rmsnorm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 0,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"num_local_experts\": 0,\n",
      "  \"output_router_logits\": false,\n",
      "  \"pad_token_id\": 100256,\n",
      "  \"position_embedding_type\": \"rope\",\n",
      "  \"residual_multiplier\": 0.263,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000000,\n",
      "  \"router_aux_loss_coef\": 0.01,\n",
      "  \"shared_intermediate_size\": 2048,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.55.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 100352\n",
      "}\n",
      "\n",
      "Rank-0 [INFO]:sft_trainer.py:train: Couldn't load model ibm-granite/granite-4.0-350M as a vision model due to Unrecognized configuration class <class 'transformers.models.granitemoehybrid.configuration_granitemoehybrid.GraniteMoeHybridConfig'> for this kind of AutoModel: AutoModelForVision2Seq.\n",
      "Model type should be one of BlipConfig, Blip2Config, ChameleonConfig, GitConfig, Idefics2Config, Idefics3Config, InstructBlipConfig, InstructBlipVideoConfig, Kosmos2Config, LlavaConfig, LlavaNextConfig, LlavaNextVideoConfig, LlavaOnevisionConfig, Mistral3Config, MllamaConfig, PaliGemmaConfig, Pix2StructConfig, Qwen2_5_VLConfig, Qwen2VLConfig, VideoLlavaConfig, VipLlavaConfig, VisionEncoderDecoderConfig. \n",
      "Rank-0 [INFO]:sft_trainer.py:train: Trying to load ibm-granite/granite-4.0-350M as language model\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--ibm-granite--granite-4.0-350M/snapshots/bd8a1497065c0d6ba1ef19af6b0d2b14bacf71c2/config.json\n",
      "Model config GraniteMoeHybridConfig {\n",
      "  \"architectures\": [\n",
      "    \"GraniteMoeHybridForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_multiplier\": 0.015625,\n",
      "  \"bos_token_id\": 100257,\n",
      "  \"embedding_multiplier\": 12,\n",
      "  \"eos_token_id\": 100257,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_method\": \"mup\",\n",
      "  \"initializer_range\": 0.1,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_types\": [\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\",\n",
      "    \"attention\"\n",
      "  ],\n",
      "  \"logits_scaling\": 4,\n",
      "  \"mamba_chunk_size\": 256,\n",
      "  \"mamba_conv_bias\": true,\n",
      "  \"mamba_d_conv\": 4,\n",
      "  \"mamba_d_head\": 16,\n",
      "  \"mamba_d_state\": 256,\n",
      "  \"mamba_expand\": 2,\n",
      "  \"mamba_n_groups\": 1,\n",
      "  \"mamba_n_heads\": 128,\n",
      "  \"mamba_proj_bias\": false,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"granitemoehybrid\",\n",
      "  \"normalization_function\": \"rmsnorm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 0,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"num_local_experts\": 0,\n",
      "  \"output_router_logits\": false,\n",
      "  \"pad_token_id\": 100256,\n",
      "  \"position_embedding_type\": \"rope\",\n",
      "  \"residual_multiplier\": 0.263,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000000,\n",
      "  \"router_aux_loss_coef\": 0.01,\n",
      "  \"shared_intermediate_size\": 2048,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.55.4\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 100352\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--ibm-granite--granite-4.0-350M/snapshots/bd8a1497065c0d6ba1ef19af6b0d2b14bacf71c2/model.safetensors\n",
      "Instantiating GraniteMoeHybridForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 100257,\n",
      "  \"eos_token_id\": 100257,\n",
      "  \"pad_token_id\": 100256,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GraniteMoeHybridForCausalLM.\n",
      "\n",
      "All the weights of GraniteMoeHybridForCausalLM were initialized from the model checkpoint at ibm-granite/granite-4.0-350M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GraniteMoeHybridForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--ibm-granite--granite-4.0-350M/snapshots/bd8a1497065c0d6ba1ef19af6b0d2b14bacf71c2/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 100257,\n",
      "  \"eos_token_id\": 100257,\n",
      "  \"pad_token_id\": 100256\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--ibm-granite--granite-4.0-350M/snapshots/bd8a1497065c0d6ba1ef19af6b0d2b14bacf71c2/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--ibm-granite--granite-4.0-350M/snapshots/bd8a1497065c0d6ba1ef19af6b0d2b14bacf71c2/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--ibm-granite--granite-4.0-350M/snapshots/bd8a1497065c0d6ba1ef19af6b0d2b14bacf71c2/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--ibm-granite--granite-4.0-350M/snapshots/bd8a1497065c0d6ba1ef19af6b0d2b14bacf71c2/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--ibm-granite--granite-4.0-350M/snapshots/bd8a1497065c0d6ba1ef19af6b0d2b14bacf71c2/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at /root/.cache/huggingface/hub/models--ibm-granite--granite-4.0-350M/snapshots/bd8a1497065c0d6ba1ef19af6b0d2b14bacf71c2/chat_template.jinja\n",
      "Rank-0 [INFO]:sft_trainer.py:train: Loaded language model as GraniteMoeHybridForCausalLM(\n",
      "  (model): GraniteMoeHybridModel(\n",
      "    (embed_tokens): Embedding(100352, 1024, padding_idx=100256)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x GraniteMoeHybridDecoderLayer(\n",
      "        (input_layernorm): GraniteMoeHybridRMSNorm((1024,), eps=1e-05)\n",
      "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1024,), eps=1e-05)\n",
      "        (shared_mlp): GraniteMoeHybridMLP(\n",
      "          (activation): SiLU()\n",
      "          (input_linear): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (output_linear): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        )\n",
      "        (self_attn): GraniteMoeHybridAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): GraniteMoeHybridRMSNorm((1024,), eps=1e-05)\n",
      "    (rotary_emb): GraniteMoeHybridRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=100352, bias=False)\n",
      ") \n",
      "Rank-0 [INFO]:sft_trainer.py:train: Loaded model tokenizer GPT2TokenizerFast(name_or_path='ibm-granite/granite-4.0-350M', vocab_size=100352, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|end_of_text|>', 'eos_token': '<|end_of_text|>', 'unk_token': '<|unk|>', 'pad_token': '<|pad|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t100256: AddedToken(\"<|pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100257: AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100258: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t100259: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t100260: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t100261: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t100262: AddedToken(\"<|filename|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t100263: AddedToken(\"<|reponame|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t100264: AddedToken(\"<|start_of_role|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100265: AddedToken(\"<|end_of_role|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100266: AddedToken(\"<|unused_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100267: AddedToken(\"<|start_of_plugin|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100268: AddedToken(\"<|end_of_plugin|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100269: AddedToken(\"<|unk|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100270: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t100271: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t100272: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t100273: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t100274: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t100275: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t100276: AddedToken(\"<think_on>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100277: AddedToken(\"<think_off>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100278: AddedToken(\"<schema>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100279: AddedToken(\"</schema>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100280: AddedToken(\"<tools>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100281: AddedToken(\"</tools>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100282: AddedToken(\"<documents>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100283: AddedToken(\"</documents>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100284: AddedToken(\"<|unused_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100285: AddedToken(\"<|unused_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100286: AddedToken(\"<|unused_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100287: AddedToken(\"<|unused_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100288: AddedToken(\"<|unused_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100289: AddedToken(\"<|unused_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100290: AddedToken(\"<|unused_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100291: AddedToken(\"<|unused_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100292: AddedToken(\"<|unused_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100293: AddedToken(\"<|unused_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100294: AddedToken(\"<|unused_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100295: AddedToken(\"<|unused_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100296: AddedToken(\"<|unused_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100297: AddedToken(\"<|unused_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100298: AddedToken(\"<|unused_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100299: AddedToken(\"<|unused_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100300: AddedToken(\"<|unused_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100301: AddedToken(\"<|unused_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100302: AddedToken(\"<|unused_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100303: AddedToken(\"<|unused_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100304: AddedToken(\"<|unused_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100305: AddedToken(\"<|unused_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100306: AddedToken(\"<|unused_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100307: AddedToken(\"<|unused_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100308: AddedToken(\"<|unused_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100309: AddedToken(\"<|unused_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100310: AddedToken(\"<|unused_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100311: AddedToken(\"<|unused_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100312: AddedToken(\"<|unused_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100313: AddedToken(\"<|unused_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100314: AddedToken(\"<|unused_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100315: AddedToken(\"<|unused_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100316: AddedToken(\"<|unused_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100317: AddedToken(\"<|unused_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100318: AddedToken(\"<|unused_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100319: AddedToken(\"<|unused_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100320: AddedToken(\"<|unused_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100321: AddedToken(\"<|unused_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100322: AddedToken(\"<|unused_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100323: AddedToken(\"<|unused_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100324: AddedToken(\"<|unused_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100325: AddedToken(\"<|unused_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100326: AddedToken(\"<|unused_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100327: AddedToken(\"<|unused_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100328: AddedToken(\"<|unused_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100329: AddedToken(\"<|unused_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100330: AddedToken(\"<|unused_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100331: AddedToken(\"<|unused_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100332: AddedToken(\"<|unused_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100333: AddedToken(\"<|unused_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100334: AddedToken(\"<|unused_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100335: AddedToken(\"<|unused_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100336: AddedToken(\"<|unused_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100337: AddedToken(\"<|unused_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100338: AddedToken(\"<|unused_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100339: AddedToken(\"<|unused_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100340: AddedToken(\"<|unused_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100341: AddedToken(\"<|unused_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100342: AddedToken(\"<|unused_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100343: AddedToken(\"<|unused_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100344: AddedToken(\"<|unused_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100345: AddedToken(\"<|unused_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100346: AddedToken(\"<|unused_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100347: AddedToken(\"<|unused_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100348: AddedToken(\"<|unused_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100349: AddedToken(\"<|unused_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100350: AddedToken(\"<|unused_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100351: AddedToken(\"<|unused_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ") \n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 100352. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "Rank-0 [INFO]:sft_trainer.py:train: Packing is set to True \n",
      "Rank-0 [INFO]:setup_dataprocessor.py:process_dataargs: Max sequence length is 1024\n",
      "Generating train split: 100%|██████████| 52002/52002 [00:00<00:00, 396184.41 examples/s]\n",
      "Rank-0 [INFO]:data_processors.py:_prepare_processed_datasets: Starting DataPreProcessor...\n",
      "Rank-0 [INFO]:data_processors.py:_prepare_processed_datasets: Loading the dataset - training_data\n",
      "Rank-0 [INFO]:data_processors.py:_prepare_processed_datasets: Loaded raw dataset : Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'text'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Rank-0 [INFO]:data_processors.py:_execute_data_handlers: setting num_proc to 8\n",
      "Rank-0 [INFO]:data_processors.py:_execute_data_handlers: Applying Handler: DataHandler(op=apply_custom_jinja_template, handler_type=MAP, allows_batching=False) Args: {'fn_kwargs': {'template': \"{{ element['text'] }}{{ eos_token }}\", 'formatted_text_column_name': 'text'}, 'batched': False, 'remove_columns': 'all', 'num_proc': 8}\n",
      "Map (num_proc=8): 100%|██████████| 1000/1000 [00:01<00:00, 702.42 examples/s]\n",
      "Rank-0 [INFO]:data_processors.py:_process_dataset_configs: Taking train split from dataset training_data\n",
      "Rank-0 [INFO]:data_processors.py:process_dataset_configs: Processed train dataset Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Rank-0 [INFO]:data_processors.py:process_dataset_configs: Processed eval dataset None\n",
      "PyTorch: setting up devices\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:412: UserWarning: Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:458: UserWarning: You are using packing, but the attention implementation is not set to 'flash_attention_2'. Packing flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` in the model configuration.\n",
      "  warnings.warn(\n",
      "Adding EOS to train dataset: 100%|██████████| 1000/1000 [00:00<00:00, 17541.39 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1000/1000 [00:00<00:00, 2714.27 examples/s]\n",
      "Packing train dataset: 100%|██████████| 1000/1000 [00:00<00:00, 14956.10 examples/s]\n",
      "***** Running training *****\n",
      "  Num examples = 97\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 97\n",
      "  Number of trainable parameters = 352,379,904\n",
      "  0%|          | 0/97 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "100%|██████████| 97/97 [03:09<00:00,  1.98s/it]Saving model checkpoint to /initializer/tmp/checkpoint-97\n",
      "Configuration saved in /initializer/tmp/checkpoint-97/config.json\n",
      "Configuration saved in /initializer/tmp/checkpoint-97/generation_config.json\n",
      "Model weights saved in /initializer/tmp/checkpoint-97/model.safetensors\n",
      "chat template saved in /initializer/tmp/checkpoint-97/chat_template.jinja\n",
      "tokenizer config file saved in /initializer/tmp/checkpoint-97/tokenizer_config.json\n",
      "Special tokens file saved in /initializer/tmp/checkpoint-97/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 97/97 [03:52<00:00,  2.40s/it]\n",
      "Rank-0 [INFO]:sft_trainer.py:save: Saving tuned model to path: /initializer/tmp\n",
      "Saving model checkpoint to /initializer/tmp\n",
      "Configuration saved in /initializer/tmp/config.json\n",
      "Configuration saved in /initializer/tmp/generation_config.json\n",
      "Model weights saved in /initializer/tmp/model.safetensors\n",
      "chat template saved in /initializer/tmp/chat_template.jinja\n",
      "tokenizer config file saved in /initializer/tmp/tokenizer_config.json\n",
      "Special tokens file saved in /initializer/tmp/special_tokens_map.json\n",
      "{'loss': 1.4856, 'grad_norm': 14.625, 'learning_rate': 9.587628865979383e-06, 'num_tokens': 5084.0, 'mean_token_accuracy': 0.6743651628494263, 'epoch': 0.05}\n",
      "{'loss': 1.3001, 'grad_norm': 14.3125, 'learning_rate': 9.072164948453609e-06, 'num_tokens': 10194.0, 'mean_token_accuracy': 0.6809111952781677, 'epoch': 0.1}\n",
      "{'loss': 1.3255, 'grad_norm': 12.125, 'learning_rate': 8.556701030927836e-06, 'num_tokens': 15293.0, 'mean_token_accuracy': 0.670821464061737, 'epoch': 0.15}\n",
      "{'loss': 1.2126, 'grad_norm': 11.5625, 'learning_rate': 8.041237113402063e-06, 'num_tokens': 20350.0, 'mean_token_accuracy': 0.7156386733055115, 'epoch': 0.21}\n",
      "{'loss': 1.1757, 'grad_norm': 14.375, 'learning_rate': 7.525773195876289e-06, 'num_tokens': 25416.0, 'mean_token_accuracy': 0.7255657553672791, 'epoch': 0.26}\n",
      "{'loss': 1.4506, 'grad_norm': 16.375, 'learning_rate': 7.010309278350515e-06, 'num_tokens': 30536.0, 'mean_token_accuracy': 0.6387096762657165, 'epoch': 0.31}\n",
      "{'loss': 1.0569, 'grad_norm': 12.4375, 'learning_rate': 6.494845360824743e-06, 'num_tokens': 35641.0, 'mean_token_accuracy': 0.7474025964736939, 'epoch': 0.36}\n",
      "{'loss': 1.1343, 'grad_norm': 10.9375, 'learning_rate': 5.979381443298969e-06, 'num_tokens': 40722.0, 'mean_token_accuracy': 0.7225459337234497, 'epoch': 0.41}\n",
      "{'loss': 1.4283, 'grad_norm': 12.6875, 'learning_rate': 5.463917525773196e-06, 'num_tokens': 45842.0, 'mean_token_accuracy': 0.6494623780250549, 'epoch': 0.46}\n",
      "{'loss': 1.4485, 'grad_norm': 11.6875, 'learning_rate': 4.948453608247423e-06, 'num_tokens': 50962.0, 'mean_token_accuracy': 0.6478983283042907, 'epoch': 0.52}\n",
      "{'loss': 1.201, 'grad_norm': 11.25, 'learning_rate': 4.4329896907216494e-06, 'num_tokens': 56060.0, 'mean_token_accuracy': 0.711926794052124, 'epoch': 0.57}\n",
      "{'loss': 1.2371, 'grad_norm': 13.4375, 'learning_rate': 3.917525773195877e-06, 'num_tokens': 61044.0, 'mean_token_accuracy': 0.7025737166404724, 'epoch': 0.62}\n",
      "{'loss': 1.2318, 'grad_norm': 12.4375, 'learning_rate': 3.4020618556701037e-06, 'num_tokens': 66111.0, 'mean_token_accuracy': 0.7090236663818359, 'epoch': 0.67}\n",
      "{'loss': 1.2831, 'grad_norm': 11.4375, 'learning_rate': 2.8865979381443297e-06, 'num_tokens': 71184.0, 'mean_token_accuracy': 0.6854410290718078, 'epoch': 0.72}\n",
      "{'loss': 1.1586, 'grad_norm': 12.125, 'learning_rate': 2.3711340206185566e-06, 'num_tokens': 76281.0, 'mean_token_accuracy': 0.7148995518684387, 'epoch': 0.77}\n",
      "{'loss': 1.2234, 'grad_norm': 13.3125, 'learning_rate': 1.8556701030927837e-06, 'num_tokens': 81368.0, 'mean_token_accuracy': 0.7062024474143982, 'epoch': 0.82}\n",
      "{'loss': 1.2292, 'grad_norm': 13.3125, 'learning_rate': 1.3402061855670104e-06, 'num_tokens': 86459.0, 'mean_token_accuracy': 0.701279878616333, 'epoch': 0.88}\n",
      "{'loss': 1.3907, 'grad_norm': 12.0625, 'learning_rate': 8.247422680412372e-07, 'num_tokens': 91549.0, 'mean_token_accuracy': 0.6576237082481384, 'epoch': 0.93}\n",
      "{'loss': 1.1436, 'grad_norm': 13.25, 'learning_rate': 3.0927835051546394e-07, 'num_tokens': 96656.0, 'mean_token_accuracy': 0.7259463906288147, 'epoch': 0.98}\n",
      "{'train_runtime': 232.6324, 'train_samples_per_second': 0.417, 'train_steps_per_second': 0.417, 'train_loss': 1.275669530494926, 'num_tokens': 98696.0, 'mean_token_accuracy': 0.6187476217746735, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "for c in client.get_job(name=job_name).steps:\n",
    "    print(f\"Step: {c.name}, Status: {c.status}, Devices: {c.device} x {c.device_count}\\n\")\n",
    "\n",
    "for line in client.get_job_logs(job_name, follow=True):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60fe3bed-91a6-4654-b3da-21ba8193ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_job(name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ebbb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Get the Fine-tuned Model\n",
    "\n",
    "After Trainer node completes the fine-tuning task, the fine-tuned model will be stored into the `/workspace/output` directory, which can be shared across Pods through PVC mounting. You can find it in another Pod's `/<mountDir>/output` directory if you mount the PVC under `/<mountDir>`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
