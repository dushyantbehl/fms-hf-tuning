{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fef6cd86-56b8-470f-b635-d312dbe14766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch():\n",
    "    import os\n",
    "\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    from torchvision import datasets, transforms\n",
    "    import torch.distributed as dist\n",
    "    from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "    # [1] Configure CPU/GPU device and distributed backend.\n",
    "    # Kubeflow Trainer will automatically configure the distributed environment.\n",
    "    device, backend = (\"cuda\", \"nccl\") if torch.cuda.is_available() else (\"cpu\", \"gloo\")\n",
    "    dist.init_process_group(backend=backend)\n",
    "\n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", 0))\n",
    "    print(\n",
    "        \"Distributed Training with WORLD_SIZE: {}, RANK: {}, LOCAL_RANK: {}.\".format(\n",
    "            dist.get_world_size(),\n",
    "            dist.get_rank(),\n",
    "            local_rank,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # [2] Define PyTorch CNN Model to be trained.\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "            self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "            self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "            self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = x.view(-1, 4 * 4 * 50)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "    # [3] Attach model to the correct device.\n",
    "    device = torch.device(f\"{device}:{local_rank}\")\n",
    "    model = nn.parallel.DistributedDataParallel(Net().to(device))\n",
    "    model.train()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "    # [4] Get the Fashion-MNIST dataset and distributed it across all available devices.\n",
    "    dataset = datasets.FashionMNIST(\n",
    "        \"./data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([transforms.ToTensor()]),\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=100,\n",
    "        sampler=DistributedSampler(dataset),\n",
    "    )\n",
    "\n",
    "    # [5] Define the training loop.\n",
    "    for epoch in range(1):\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # Attach tensors to the device.\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = F.nll_loss(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0 and dist.get_rank() == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(inputs),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    # Wait for the training to complete and destroy to PyTorch distributed process group.\n",
    "    dist.barrier()\n",
    "    if dist.get_rank() == 0:\n",
    "        print(\"Training is finished\")\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d28c7ff2-277c-4f82-b4b8-141dc310774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Runtime mpi-distributed must have trainer.kubeflow.org/framework label.\n",
      "Runtime torchtune-llama3.2-1b must have trainer.kubeflow.org/framework label.\n",
      "Runtime torchtune-llama3.2-3b must have trainer.kubeflow.org/framework label.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: deepspeed-distributed\n",
      "Runtime: mlx-distributed\n",
      "Runtime: torch-distributed\n",
      "Runtime: torchtune-qwen2.5-1.5b\n"
     ]
    }
   ],
   "source": [
    "from kubeflow.trainer import TrainerClient, CustomTrainer\n",
    "\n",
    "for r in TrainerClient().list_runtimes():\n",
    "    print(f\"Runtime: {r.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acab6337-d77c-491a-ad24-de0c616ffc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = TrainerClient().train(\n",
    "    trainer=CustomTrainer(\n",
    "        func=train_pytorch,\n",
    "        num_nodes=1,\n",
    "        resources_per_node={\n",
    "            \"cpu\": 1,\n",
    "            \"memory\": \"4Gi\",\n",
    "            \"gpu\": 1, # Comment this line if you don't have GPUs.\n",
    "        },\n",
    "    ),\n",
    "    runtime=TrainerClient().get_runtime(\"torch-distributed\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "147b8826-7e09-4768-a4dd-d4c4bb92c127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TrainJob(name='s5ae0235f02c', creation_timestamp=datetime.datetime(2025, 11, 7, 18, 42, 36, tzinfo=TzInfo(0)), runtime=Runtime(name='torch-distributed', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='torch', num_nodes=1, device='Unknown', device_count='Unknown'), pretrained_model=None), steps=[Step(name='node-0', status='Running', pod_name='s5ae0235f02c-node-0-0-sl6lp', device='gpu', device_count='1')], num_nodes=1, status='Running')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainerClient().list_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d226526-df18-4d4a-b26b-1dc7b69e3e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: node-0, Status: Running, Devices: gpu x 1\n"
     ]
    }
   ],
   "source": [
    "for s in TrainerClient().get_job(name=job_id).steps:\n",
    "    print(f\"Step: {s.name}, Status: {s.status}, Devices: {s.device} x {s.device_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5431793b-a4c2-49dd-9cba-19d406f4d064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributed Training with WORLD_SIZE: 1, RANK: 0, LOCAL_RANK: 0.\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n",
      "100%|███████���██| 26421880/26421880 [00:32<00:00, 819422.47it/s]\n",
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
      "100%|██████████| 29515/29515 [00:00<00:00, 192512.06it/s]\n",
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "100%|██████████| 4422102/4422102 [00:01<00:00, 3157200.97it/s]\n",
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "100%|██████████| 5148/5148 [00:00<00:00, 3510939.35it/s]\n",
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.312296\n",
      "Train Epoch: 0 [1000/60000 (2%)]\tLoss: 2.092805\n",
      "Train Epoch: 0 [2000/60000 (3%)]\tLoss: 1.597475\n",
      "Train Epoch: 0 [3000/60000 (5%)]\tLoss: 1.288376\n",
      "Train Epoch: 0 [4000/60000 (7%)]\tLoss: 0.876789\n",
      "Train Epoch: 0 [5000/60000 (8%)]\tLoss: 0.726644\n",
      "Train Epoch: 0 [6000/60000 (10%)]\tLoss: 0.593700\n",
      "Train Epoch: 0 [7000/60000 (12%)]\tLoss: 0.722750\n",
      "Train Epoch: 0 [8000/60000 (13%)]\tLoss: 0.594839\n",
      "Train Epoch: 0 [9000/60000 (15%)]\tLoss: 0.675138\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.603724\n",
      "Train Epoch: 0 [11000/60000 (18%)]\tLoss: 0.582024\n",
      "Train Epoch: 0 [12000/60000 (20%)]\tLoss: 0.596295\n",
      "Train Epoch: 0 [13000/60000 (22%)]\tLoss: 0.436081\n",
      "Train Epoch: 0 [14000/60000 (23%)]\tLoss: 0.621243\n",
      "Train Epoch: 0 [15000/60000 (25%)]\tLoss: 0.675425\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.496636\n",
      "Train Epoch: 0 [17000/60000 (28%)]\tLoss: 0.483333\n",
      "Train Epoch: 0 [18000/60000 (30%)]\tLoss: 0.543964\n",
      "Train Epoch: 0 [19000/60000 (32%)]\tLoss: 0.690950\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.493640\n",
      "Train Epoch: 0 [21000/60000 (35%)]\tLoss: 0.462233\n",
      "Train Epoch: 0 [22000/60000 (37%)]\tLoss: 0.370234\n",
      "Train Epoch: 0 [23000/60000 (38%)]\tLoss: 0.564543\n",
      "Train Epoch: 0 [24000/60000 (40%)]\tLoss: 0.361542\n",
      "Train Epoch: 0 [25000/60000 (42%)]\tLoss: 0.400380\n",
      "Train Epoch: 0 [26000/60000 (43%)]\tLoss: 0.557234\n",
      "Train Epoch: 0 [27000/60000 (45%)]\tLoss: 0.399511\n",
      "Train Epoch: 0 [28000/60000 (47%)]\tLoss: 0.466581\n",
      "Train Epoch: 0 [29000/60000 (48%)]\tLoss: 0.445299\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.473450\n",
      "Train Epoch: 0 [31000/60000 (52%)]\tLoss: 0.456349\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.418910\n",
      "Train Epoch: 0 [33000/60000 (55%)]\tLoss: 0.286719\n",
      "Train Epoch: 0 [34000/60000 (57%)]\tLoss: 0.505181\n",
      "Train Epoch: 0 [35000/60000 (58%)]\tLoss: 0.569575\n",
      "Train Epoch: 0 [36000/60000 (60%)]\tLoss: 0.378841\n",
      "Train Epoch: 0 [37000/60000 (62%)]\tLoss: 0.375278\n",
      "Train Epoch: 0 [38000/60000 (63%)]\tLoss: 0.474018\n",
      "Train Epoch: 0 [39000/60000 (65%)]\tLoss: 0.354459\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.447489\n",
      "Train Epoch: 0 [41000/60000 (68%)]\tLoss: 0.540134\n",
      "Train Epoch: 0 [42000/60000 (70%)]\tLoss: 0.435562\n",
      "Train Epoch: 0 [43000/60000 (72%)]\tLoss: 0.252390\n",
      "Train Epoch: 0 [44000/60000 (73%)]\tLoss: 0.287184\n",
      "Train Epoch: 0 [45000/60000 (75%)]\tLoss: 0.224768\n",
      "Train Epoch: 0 [46000/60000 (77%)]\tLoss: 0.505465\n",
      "Train Epoch: 0 [47000/60000 (78%)]\tLoss: 0.379566\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.576883\n",
      "Train Epoch: 0 [49000/60000 (82%)]\tLoss: 0.401864\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.345336\n",
      "Train Epoch: 0 [51000/60000 (85%)]\tLoss: 0.419392\n",
      "Train Epoch: 0 [52000/60000 (87%)]\tLoss: 0.399398\n",
      "Train Epoch: 0 [53000/60000 (88%)]\tLoss: 0.364954\n",
      "Train Epoch: 0 [54000/60000 (90%)]\tLoss: 0.345955\n",
      "Train Epoch: 0 [55000/60000 (92%)]\tLoss: 0.447523\n",
      "Train Epoch: 0 [56000/60000 (93%)]\tLoss: 0.392146\n",
      "Train Epoch: 0 [57000/60000 (95%)]\tLoss: 0.409204\n",
      "Train Epoch: 0 [58000/60000 (97%)]\tLoss: 0.367196\n",
      "Train Epoch: 0 [59000/60000 (98%)]\tLoss: 0.224907\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.344910\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 0.506135\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 0.264765\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 0.346366\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.379166\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.307894\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.371602\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 0.415608\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.351359\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 0.419408\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.366946\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 0.320495\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.430746\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 0.351931\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.470155\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.402428\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.283073\n",
      "Train Epoch: 1 [17000/60000 (28%)]\tLoss: 0.381417\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.252019\n",
      "Train Epoch: 1 [19000/60000 (32%)]\tLoss: 0.474756\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.371643\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 0.209806\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 0.282521\n",
      "Train Epoch: 1 [23000/60000 (38%)]\tLoss: 0.404907\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.227012\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.308363\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.368788\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 0.323326\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.319397\n",
      "Train Epoch: 1 [29000/60000 (48%)]\tLoss: 0.280091\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.382552\n",
      "Train Epoch: 1 [31000/60000 (52%)]\tLoss: 0.343119\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.380451\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 0.202792\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.437034\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.508283\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.336430\n",
      "Train Epoch: 1 [37000/60000 (62%)]\tLoss: 0.349311\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.321455\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 0.311600\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.213058\n",
      "Train Epoch: 1 [41000/60000 (68%)]\tLoss: 0.444061\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.320155\n",
      "Train Epoch: 1 [43000/60000 (72%)]\tLoss: 0.253580\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.305396\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.214360\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.381652\n",
      "Train Epoch: 1 [47000/60000 (78%)]\tLoss: 0.276878\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.538030\n",
      "Train Epoch: 1 [49000/60000 (82%)]\tLoss: 0.251471\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.218167\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 0.438873\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.309049\n",
      "Train Epoch: 1 [53000/60000 (88%)]\tLoss: 0.263821\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.287074\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.336322\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.343655\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 0.314571\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.361520\n",
      "Train Epoch: 1 [59000/60000 (98%)]\tLoss: 0.198540\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.367811\n",
      "Train Epoch: 2 [1000/60000 (2%)]\tLoss: 0.399965\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.305855\n",
      "Train Epoch: 2 [3000/60000 (5%)]\tLoss: 0.275284\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.386688\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.367547\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.393205\n",
      "Train Epoch: 2 [7000/60000 (12%)]\tLoss: 0.305327\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.322322\n",
      "Train Epoch: 2 [9000/60000 (15%)]\tLoss: 0.335645\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.265156\n",
      "Train Epoch: 2 [11000/60000 (18%)]\tLoss: 0.305411\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.305350\n",
      "Train Epoch: 2 [13000/60000 (22%)]\tLoss: 0.278717\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.377531\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.335196\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.215454\n",
      "Train Epoch: 2 [17000/60000 (28%)]\tLoss: 0.283098\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.266231\n",
      "Train Epoch: 2 [19000/60000 (32%)]\tLoss: 0.371816\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.293932\n",
      "Train Epoch: 2 [21000/60000 (35%)]\tLoss: 0.196384\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.213199\n",
      "Train Epoch: 2 [23000/60000 (38%)]\tLoss: 0.441535\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.222536\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.226926\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.415824\n",
      "Train Epoch: 2 [27000/60000 (45%)]\tLoss: 0.218164\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.335454\n",
      "Train Epoch: 2 [29000/60000 (48%)]\tLoss: 0.263154\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.285830\n",
      "Train Epoch: 2 [31000/60000 (52%)]\tLoss: 0.327936\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.310273\n",
      "Train Epoch: 2 [33000/60000 (55%)]\tLoss: 0.202903\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.409390\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.398169\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.299961\n",
      "Train Epoch: 2 [37000/60000 (62%)]\tLoss: 0.214509\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.257384\n",
      "Train Epoch: 2 [39000/60000 (65%)]\tLoss: 0.274457\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.186123\n",
      "Train Epoch: 2 [41000/60000 (68%)]\tLoss: 0.412746\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.304954\n",
      "Train Epoch: 2 [43000/60000 (72%)]\tLoss: 0.197798\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.253427\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.188451\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.351015\n",
      "Train Epoch: 2 [47000/60000 (78%)]\tLoss: 0.255767\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.462039\n",
      "Train Epoch: 2 [49000/60000 (82%)]\tLoss: 0.245289\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.172763\n",
      "Train Epoch: 2 [51000/60000 (85%)]\tLoss: 0.362138\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.263599\n",
      "Train Epoch: 2 [53000/60000 (88%)]\tLoss: 0.281908\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.264708\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.320572\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.387519\n",
      "Train Epoch: 2 [57000/60000 (95%)]\tLoss: 0.353611\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.290043\n",
      "Train Epoch: 2 [59000/60000 (98%)]\tLoss: 0.160975\n",
      "Training is finished\n"
     ]
    }
   ],
   "source": [
    "for logline in TrainerClient().get_job_logs(job_id, follow=True):\n",
    "    print(logline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd9179c4-6fd6-436d-8261-e52aa9750a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in TrainerClient().list_jobs():\n",
    "    name = j.name \n",
    "    TrainerClient().delete_job(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1639327b-70c6-4cac-9dad-c03b82278da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
